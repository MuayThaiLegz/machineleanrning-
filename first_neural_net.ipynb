{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b613425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6a4ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "from pylab import mpl, plt\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8b19a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist \n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92ea692d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_images and train_labels form the training set, the data that the model will learn from.\n",
    "The Model will be tested on the test set, test_images and test_labels.\n",
    "\n",
    "The images are encoded as np arrays, and the labels are an array of digits, ranging from 0:9\n",
    "Images and labels have a one-to-one correspondence\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(train_images.shape)\n",
    "\n",
    "display(len(train_labels))\n",
    "\n",
    "display(test_images.shape)\n",
    "\n",
    "display(len(test_labels))\n",
    "\n",
    "display(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac27759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The network architecture.\n",
    "The core building block of neural networks is the layer, a data-processing module that you can think of as a filter for data\n",
    "Data does in and comes out in a more usefull form. Layers extract representations out of the data fed into them.\n",
    "Most deep learning consists of chaining together simple layers that will implement a for, of progressive data distillation.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "\n",
    "# dense|fully connected neural layer. \n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "# a 10-way layer, will return an array of 10 probabikity scores summing to 1.\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34166bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To make the network ready for training, we need to pick three more things, as part \n",
    "of the compilation step:\n",
    "\n",
    "* A loss function -- How the network will be able to measure its performance on the training data, and thus how it will \n",
    "    be able to steer itself in the right direction.\n",
    "\n",
    "* An optimizer -- The mechanism through which the network updates itself based on the data it sees and its loss function\n",
    "\n",
    "* Metrics to monitor during training and testing -- Here, we'll only care about accuracy.\n",
    "    (fraction of images correctly classified).\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72eca869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before training, we'll preprocess the data by reshaping it into the shape the network expects and scaling it\n",
    "so the all values are in the [0, 1] interval. Previosly, our training images, for instance,\n",
    "were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval.\n",
    "We transform it into a float32 array of shape (60000. 28 * 28) with values between 0 and 1.\n",
    "\"\"\"\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "train_images = train_images.astype('float') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbbf5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "\n",
    "test_images = test_images.astype('float') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c175296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to categorically encode the labels\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3fa05a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2586 - acc: 0.9258\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1044 - acc: 0.9691\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0694 - acc: 0.9796\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0499 - acc: 0.9849\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0381 - acc: 0.9883\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0293 - acc: 0.9914\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0225 - acc: 0.9936\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0170 - acc: 0.9952\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0105 - acc: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9cab13ac8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are now ready to train the network which in keras is done via a call to the networks fir metthod.\n",
    "# We fit the model to its training data\n",
    "\n",
    "network.fit(train_images, train_labels,epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33f34431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 18us/sample - loss: 0.0737 - acc: 0.9815\n",
      "test_acc: 0.9815\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two quantities are displayed during training: the loss of the network over the training data,\n",
    "and the accuracy of the network over the training data.\n",
    "\n",
    "\n",
    "Let's check that the model performs well on the test set, too:\n",
    "\"\"\"\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "\n",
    "print('test_acc:',  test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767be3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6cf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5531f17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([12,  3,  6, 14])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5, 78,  2, 34,  0],\n",
       "       [ 6, 79,  3, 35,  1],\n",
       "       [ 7, 80,  4, 36,  2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[list([5, 78, 2, 34, 0]), list([6, 79, 3, 35, 1]),\n",
       "        list([7, 80, 4, 36, 2])],\n",
       "       [list([5, 78, 2, 34, 0]),\n",
       "        list([6, 79, 3, 35, 1, [7, 80, 4, 36, 2]]),\n",
       "        list([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]])]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Data representaions for neural networks\n",
    "In general, all current machine learning systems use tensors as their basic data structure. \n",
    "\n",
    "Tensor: at its core, a tensor is a container for data-- almost always numerical data. So, it's a container for numbers.\n",
    "Matrices are2d tensors; tensors are a generelization of matrices to an arbitrary number of dimensions.\n",
    "(note that in the context of tensors, a dimension is often called an axis)\n",
    "\n",
    "\n",
    "\n",
    "# Scalars (OD tensors)\n",
    "A tensor that contains only one number is called a scalar(or scalar tensor, or 0-dimensional tensor, or 0D tensor). In Numpy\n",
    "a float32 or float64 number is a scalar tensor (or scalar array). You can display the number of axes of a numpy tensor viaa\n",
    "the ndim attribute; a scalar tensor has 0 axes (nim == 0).The number of axes of a tensor is also called its rank.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "x = np.array(12)\n",
    "\n",
    "display(x)\n",
    "\n",
    "display(x.ndim)\n",
    "\n",
    "# Vectors (1d tensors)\n",
    "# An array of numbers is called a vector, or 1d tensor. A 1d tensor is said to hace exactly one axis\n",
    "\n",
    "xx= np.array([12, 3, 6, 14])\n",
    "\n",
    "display(xx)\n",
    "\n",
    "display(xx.ndim)\n",
    "\n",
    "# This vector has five entries and so is called a 5-dimensional vector. Dont confuse a 5d vector with a 5d tesnor!\n",
    "# A 5d vector has only one axis and has five dimensions along its axis,\n",
    "# A 5d tensor has five axes (and may have any number of dimensions along each axis)\n",
    "# Dimensionality can denote either the number of entries along a specific axis(as in case of 5d vector)\n",
    "# or the umber of axes in a tensor (such as in a 5d tensor), \n",
    "\n",
    "# Matrices (2D tensors)\n",
    "\n",
    "# An array of vectors is a matrix, or 2d tensor. A matrix has two axes (often referred to rows and colunms).\n",
    "# you can visulaly interpert a matrix a as a rectangular grid of numbers.\n",
    "\n",
    "x_ma = np.array([[5, 78, 2, 34, 0],\n",
    "                [6, 79, 3, 35, 1],\n",
    "                [7, 80, 4, 36, 2]])\n",
    "\n",
    "display(x_ma)\n",
    "\n",
    "display(x_ma.ndim)\n",
    "\n",
    "# 3D tensors and higer dimensional tensors\n",
    "\n",
    "# if you pack such matrices into a new array, you obtain a 3d tensor\n",
    "\n",
    "xDDD = np.array([[[5, 78, 2, 34, 0],\n",
    "                 [6, 79, 3, 35, 1],\n",
    "                 [7, 80, 4, 36, 2]],\n",
    "                [[5, 78, 2, 34, 0],\n",
    "                 [6, 79, 3, 35, 1,\n",
    "                 [7, 80, 4, 36, 2]],\n",
    "                [[5, 78, 2, 34, 0],\n",
    "                 [6, 79, 3, 35, 1],\n",
    "                 [7, 80, 4, 36, 2]]]])\n",
    "\n",
    "display(xDDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9dea01aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Key attributes: A tensor is defined by three key atrributes:\n",
    "\n",
    "* Number of axes (rank) - For instance, a 3D tensor has three axes, and a matrix has two axes.\n",
    "This is also called the tensor's ndim.\n",
    "\n",
    "* Shape - This is a tubple of intergers that describes how many dimensions the tensor has along each axis.\n",
    "For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5).\n",
    "A vector has s shape with s single element, such as (5,), whereas a scalar has en empty shape,().\n",
    "\n",
    "* Data type (usually called dtype in py libs) - This is the type of the data caontained in the tensor; for instancce,\n",
    "a tensor;s type could be float32, uint8, and so on. \n",
    "\n",
    "NOTE - Strings dont live in preallocated, contiguous momoey segments:\n",
    "    and strings, being variable length, would preclude the use of this implementaion.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.datasets import mnist \n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# displaying image\n",
    "\n",
    "digit = train_images[4]\n",
    "\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8eb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Manipulation of tensors in Numpy:\n",
    "Selecting specific elements in a tensor is called tensor slicing.\n",
    "The tensor-slicing operations you can do on Numpy arrays\n",
    "\"\"\"\n",
    "\n",
    "# selects digits #10 to #100 (#100 is not included) puts them in an array shape (90, 28, 28)\n",
    "\n",
    "my_slice = train_images[10:100]\n",
    "\n",
    "print(my_slice.shape)\n",
    "\n",
    "# Equivalnnet \n",
    "\n",
    "my_slice_eq = train_images[10:100, :, :]\n",
    "\n",
    "display(my_slice_eq.shape)\n",
    "\n",
    "# also rquvialent to the previous example \n",
    "\n",
    "my_slice_22 = train_images[10:100, 0:28, 0:28]\n",
    "\n",
    "display(my_slice_22.shape)\n",
    "\n",
    "\n",
    "# select betwwen any two indices along each tensor axis. To selelect 14 x 14 pixels centered in the middle, you do this:\n",
    "\n",
    "myslice = train_images[:, 7:-7, 7:-7]\n",
    "\n",
    "plt.imshow(myslice[9], cmap=plt.cm.binary)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# The notion of data batches: In general, the first axis )axis 0, because indexing starts at 0) in all data tensors you'll \n",
    "# come across in deep learning will be the samples aaxis (the samples dimension) \n",
    "# deep learning model doent process an entire daset at once; rather, they break the data into small batches.\n",
    "# Concretely, here's one batch of our MNIST digits, with batch size of 218:\n",
    "\n",
    "batch = train_images[:128]\n",
    "\n",
    "# And here's the next batch:\n",
    "\n",
    "batchDos = train_images[128:256]\n",
    "\n",
    "# and the nth batch:\n",
    "\n",
    "#batchN = train_images[128 * n:128 * (n + 1)]\n",
    "\n",
    "# display(batchN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d109cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# When considering such a batch tensor, the first axis (axis 0) is called the batch axis or batch dimension.\n",
    "# This is a term you'll  frequently encounter when using Keras and other deep-learning-libraries.\n",
    "\n",
    "# Real-world examples of data tensors\n",
    "# Data you'll manipulate will almost always fall into one of the following categories:\n",
    "\n",
    "# Vector data - 2D tensors of shape (sample, features)\n",
    "\n",
    "# Time series data or sequence data - 3D tensors of shape (samples, timesteps, features)\n",
    "\n",
    "#Images - 4D tesnors of shape (samples, height, width, channels) or (samples, channels, height, width)\n",
    "\n",
    "# Video - 5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)\n",
    "\n",
    "# Vector data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5474a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiave_relu(x):\n",
    "    assert len(x.shape) ==2\n",
    "    \n",
    "    x = x.copy\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40beefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape(x.shape[1])):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a0e3f7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_63700/4115161775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m  \u001b[1;31m# Element-wise addition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "#z = x + y  # Element-wise addition\n",
    "#z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a73846ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vecotr(x, y):\n",
    "    assert len(x.shape) == 2 # x is a 2d Numpy tensor \n",
    "    assert len(y.shape) == 1 # y is a numpy vector\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy() # Avoid oversriting the input tesor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i]\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "579bd394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75075778, 0.7235887 , 0.69396748, 0.32392544, 0.73920156,\n",
       "       0.3277758 , 0.30852156, 0.98417978, 0.48482687, 0.60765964])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "With broadcasting, you can genrally apply two-tensor element-wise oprations if one tensor has shape\n",
    "(a, b, ... n , n + 1, ...m) and the other shape (n, n + 1, .. m).\n",
    "the broadcasting will then automatically happen for axes a through n -1.\n",
    "    The following example applies the element-wise maximum operaion to two tensors of different shapes via broadcasting:\n",
    "\"\"\"\n",
    "\n",
    "x = np.random.random((64, 3, 32, 10)) # x is a random tensor with shape (54, 3, 32, 10)\n",
    "\n",
    "y = np.random.random((32, 10)) # y is a random tensor with shape (32, 10)\n",
    "\n",
    "z = np.maximum(x, y)  # The output z has shape (64, 3, 32, 10) like x\n",
    "\n",
    "display(z[0][0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "340d4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensor dot: The dot operation, also called a tensor product (not to be confused with an element-wise product)\n",
    "is the most common, most useful tensor operation. Contray to elemnt-wise operations,\n",
    "it combines entries in the input tensors. An element-wise product is done with the * operator in Numpy, Keras, Theano & Tf.\n",
    "dot uses a diffrent syntax in tf, but in both numpy and keras its done using the standard dot operator\n",
    "\"\"\"\n",
    "x = 5 \n",
    "y = 4\n",
    "\n",
    "z = np.dot(x, y)\n",
    "\n",
    "#z = x . y\n",
    "\n",
    "# In mathematical notation, you'd note the operation with a dot (.):\n",
    "\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1 \n",
    "    assert len(y.shape) == 1 \n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0 \n",
    "    for i in range(x.range[0]):\n",
    "        x += x[i] * y[i]\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf16f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Tensor reshaping:\n",
    "A third type of tensor operation that's essential to understand is tensor reshaping>\n",
    "Used when we preprocess digit data before feeding it our network:\n",
    "\"\"\"\n",
    "\n",
    "# train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "\n",
    "display(x.shape)\n",
    "\n",
    "xx = x.reshape((6, 1))\n",
    "\n",
    "display(xx)\n",
    "\n",
    "xxx = x.reshape((2, 3))\n",
    "\n",
    "display(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1866245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "# A special case of reshaping that's commonly encountered is transposition. Transposing a matrix means exchanging its rows\n",
    "# and its columns, so that x[i, :]: becomes x[:, i]:\n",
    "\n",
    "x = np.zeros((300, 20))\n",
    "\n",
    "x = np.transpose(x)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47b06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67671fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 3 Getting started with neural networks\n",
    "\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "layer = layers.Dense(32, input_shape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "70a0eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = models.Sequential()\n",
    "\n",
    "mod.add(layers.Dense(32,\n",
    "                     activation= 'relu',\n",
    "                     input_shape=(784,)))\n",
    "\n",
    "mod.add(layers.Dense(10,\n",
    "                     activation='softmax'))\n",
    "\n",
    "# and here's tje same model defined using the functional API:\n",
    "\n",
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu') (input_tensor)\n",
    "output_tensor = layers.Dense(10,activation='softmax')(x)\n",
    "model = models.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Once architecture is made it doesnt matter whether you used a sequential mod or func api. All next steps are the same.\n",
    "# The learning process is configured in the complitation step. where you specify the optimiser and loss fucntion(s)\n",
    "# that the model should usem as well as the mstrrics you want to monitor during training.\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "mod.compile(optimizer = optimizers.RMSprop(lr=0.001),\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# the learning process consists of passing Numpy arrays of imput data (and the correspinging target data) to the model via fit()\n",
    "\n",
    "# mod.fit(input_tensor, output_tensor, batch_size=128, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25894cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
